---
layout: post
title: ベイズ階層言語モデルによる教師なし形態素解析 [NPYLM]
category: 論文
tags:
- 自然言語処理
- VPYLM
- HPYLM
excerpt_separator: <!--more-->
---

## 概要

- [ベイズ階層言語モデルによる教師なし形態素解析](http://chasen.org/~daiti-m/paper/nl190segment.pdf)を読んだ
- NPYLMの実装方法の解説

<!--more-->

## はじめに

NPYLMは文字nグラムモデルと単語nグラムモデルを教師なし学習によって同時に学習を行うモデルです。

文字nグラムモデルは文字列の並びに適切な確率を与えるモデルです。

例えばbrowに続いてnが出現する確率は文字nグラムモデルによって与えられます。

$$
	\begin{align}
		p(n \mid brow)
	\end{align}\
$$

単語nグラムモデルは単語の並びに適切な確率を与えるモデルです。

the quick brownに続いてfoxが出現する確率は単語nグラムモデルによって与えられます。

$$
	\begin{align}
		p(fox \mid the\ quick\ brown)
	\end{align}\
$$

NPYLMではこれらのモデルを、分かち書きされていない文字列データから教師なしで学習します。

単語nグラムの学習の際に文字列を単語列に分かち書きする必要があり、その操作が「教師なし形態素解析」にあたります。

NPYLMは言語モデルであり、形態素解析手法ではありませんので注意が必要です。

## 参考文献

読んでおいたほうがいい文献です。

- [A Bayesian Interpretation of Interpolated Kneser-Ney](https://www.stats.ox.ac.uk/~teh/research/compling/hpylm.pdf)
- [Pitman-Yor過程に基づく可変長n-gram 言語モデル](http://chasen.org/~daiti-m/paper/nl178vpylm.pdf)
- [ベイズ階層言語モデルによる教師なし形態素解析](http://chasen.org/~daiti-m/paper/nl190segment-slides.pdf)
- [教師なし形態素解析とその周辺](http://chasen.org/~daiti-m/paper/Robotics-20130327.pdf)
- [理論はどうでもいいから作ってみたい人のためのNPYLM](http://www.slideshare.net/uchumik/dsirnlp06-nested-pitmanyor-language-model)

## おことわり

私は自然言語処理を勉強したことがないため、この分野の慣例などは全く知りません。

論文の理解が間違っている可能性があるためご了承下さい。

また特許の関係でソースコードは公開できません。

## ベイズ階層nグラム言語モデル

NPYLMを実装するにあたって、その根幹をなすベイズ階層言語モデルを理解する必要があります。

### nグラム分布の階層的な生成

ユニグラム（1グラム）分布$G_1 = p(\cdot)$があるとき、ある単語$w$を文脈とするバイグラム（2グラム）分布$G_2 = p(\cdot \mid w)$は、$G_1$とは異なるものの高頻度語などについて$G_1$を反映していると考えられます。

同様にしてトライグラム（3グラム）分布$G_3 = P(\cdot \mid w_1w_2)$は$G_2$を反映し、4グラム分布$G_4 = P(\cdot \mid w_1w_2w_3)$は$G_3$を反映し、・・・

つまり、nグラム分布はn-1グラム分布を反映したものになっていると仮定します。

このような親の分布を反映した新しい分布を生成する仕組みとしてディリクレ過程やPitman-Yor過程があり、NPYLMではPitman-Yor過程に基づくnグラムモデルを用います。

（ディリクレ過程に基づくモデルも作ろうと思えば作れますが、自然言語がもつ"べき乗則"に従わないため、言語モデルにはPitman-Yor過程が使われます）

これらの確率過程については[続・わかりやすいパターン認識](http://amzn.asia/fauVKjZ)などの書籍に非常によくまとまっているため読むことをおすすめします。

### 中華料理店過程（CRP）

CRPはディリクレ過程の実現例の一つであり、クラスタリングの事前確率をモデル化する手法です。

これはどういうことかと言うと、まだデータを観測していない状態で、もしデータをクラスタリングするとしたらどのような分割が起こりうるか？ということをモデル化するものです。

自然言語処理においては、ある単語を観測する以前の状態のときに、これから観測する単語が何なのかをモデル化するということです。

CRPでは、テーブル、客、集中度パラメータ$\theta$を考えます。

単語の出現回数をテーブルと客で管理し、テーブル=単語、客数=その単語が生成された回数を表します。

$n$番目以降の客は、

- すでに$n_i$人着席しているテーブル$i$に確率$n_i/(n-1+\theta)$で着席する
- 新しいテーブルに確率$\theta/(n-1+\theta)$で着席する

のルールにもとづいてテーブルに着席します。

以下の図は7人目の客が座るテーブルを決定するそれぞれの確率を示しています。

![crp](/images/post/2016-12-11/crp.png)

この図には客は6人いますが、これは単語を観測する以前の状態で、単語を6個観測したと仮定すると、それら6個の単語はそれぞれ何なのかを表しています。

図より単語1を3回、単語2を2回、単語3を1回観測するであろうということがわかります。

ちなみに同じテーブルが複数存在してもかまいません。

特に$G_0$からテーブルが生成されるときはすでにあるテーブルと同じものが生成されることがあります。

次に、$n$番目までの客を観測した状態での$n+1$番目の客$x_{n+1}$が座るテーブルの予測分布を考えます。

これは以下のように各テーブルの客からなる経験分布と基底測度$G_0$の混合分布になります。

$$
	\begin{align}
		p(x_{n+1} \mid x_1...x_n) = \sum_{k=1}^{t_{\cdot}}\frac{c_k}{c_{\cdot}+\theta} + \frac{\theta}{c_{\cdot}+\theta}G_0
	\end{align}\
$$

（$t_{\cdot}$は総テーブル数、$c_{\cdot}$は総客数、$c_k$は$k$番目のテーブルの客数）

ここで基底測度$G_0$が出てきましたが、これはテーブルを生成する親の連続分布です。一様分布を考えるとわかりやすいと思います。

この分布は、客$x_{n+1}$の座るテーブルが、$\frac{c_k}{c_{\cdot}+\theta}$の比で$k$番目のテーブルになり、$\frac{\theta}{c_{\cdot}+\theta}$の比で$G_0$から生成された新しいテーブルになることを表しています。

この分布から客$x_{n+1}$の座るテーブルが$w$になる確率を求めることができ、以下のようになります。

$$
	\begin{align}
		p(x_{n+1} = w \mid \Theta) = \frac{c_w}{c_{\cdot}+\theta} + \frac{\theta}{c_{\cdot}+\theta}G_0
	\end{align}\
$$

（$c_w$はテーブル$w$の客数、$\Theta$は$n$番目の客が着席した時点の客とテーブルの配置）

いま$G_0$が一様分布だと仮定します。

この時、$\theta$が大きければ$p(x_{n+1} = w \mid \Theta)$は$G_0$に近づくため、一様分布に近い分布になります。

逆に$\theta$が小さければ$p(x_{n+1} = w \mid \Theta)$は経験分布$\frac{c_w}{c_{\cdot}+\theta}$に近い分布になります。

このようにCRP（ディリクレ過程）は集中度パラメータ$\theta$と基底測度$G_0$から新しい分布$G$を生成することができ、$\theta$を調整することによって$G$を$G_0$に似せたり違う分布にしたりすることができます。

これは通常以下のように書きます。

$$
	\begin{align}
		G \sim DP(G_0, \theta)
	\end{align}\
$$

（$DP$はディリクレ過程を表します）

ちなみに$\theta \to \infty$では$G$は$G_0$に一致します。

### Pitman-Yor過程

Pitman-Yor過程はCRPにディスカウント係数$d$を足したものになっており、基本はCRPと変わりません。

着席の際、$n$番目以降の客は、

- すでに$n_i$人着席しているテーブル$i$に確率$(n_i-d)/(n-1+\theta)$で着席する
- 新しいテーブルに確率$(\theta+dt_{\cdot})/(n-1+\theta)$で着席する

のルールにもとづいてテーブルに着席します。

ただし$t_{\cdot}$は$n-1$番目の客が着席した時点での総テーブル数を表します。

![pitman-yor](/images/post/2016-12-11/pitman-yor.png)

生成される分布$G$は

$$
	\begin{align}
		G \sim PY(G_0, \theta, d)
	\end{align}\
$$

のように書きます。

CRPと同様に予測分布は以下のようになります。

$$
	\begin{align}
		p(x_{n+1} \mid x_1...x_n) = \sum_{k=1}^{t_{\cdot}}\frac{c_k-d}{c_{\cdot}+\theta} + \frac{\theta+dt_{\cdot}}{c_{\cdot}+\theta}G_0\\
		p(x_{n+1} = w \mid \Theta) = \frac{c_w-dt_w}{c_{\cdot}+\theta} + \frac{\theta+dt_{\cdot}}{c_{\cdot}+\theta}G_0
	\end{align}\
$$

結局のところCRP（ディリクレ過程）やPitman-Yor過程がやっていることは、基底測度$G_0$に似た分布を作っているだけです。

そのことを理解しておけば問題はありません。

なお、ディリクレ過程とPitman-Yor過程の違いは新しいテーブルの生成のされ方にあります。

ディリクレ過程では使用テーブル数は来客数$n$に対して${\rm log}n$のオーダーで増えていきます。

一方Pitman-Yor過程では使用テーブル数は$n^d$のオーダーで増えていきます。

一般に2変数$x,y$の間に$y=ax^b$の関係があるとき、"べき乗則"に従うと言います。（$a,b$は定数）

Pitman-Yor過程の使用テーブル数はべき乗則に従っており、来客数が増えるほど生成されるテーブルの種類の幅が広がります。

これは自然言語における低頻度語のロングテール現象に合うため、言語モデルにはPitman-Yor過程が使われます。

### 階層Pitman-Yor過程

Pitman-Yor過程の$G_0$をまた別のPitman-Yor過程にすることで階層Pitman-Yor過程を作ることができます。

先ほどnグラム分布はn-1グラム分布を反映していると仮定しましたが、階層Pitman-Yor仮定の枠組みでnグラムモデルを考えると以下の図のようになります。

![nested](/images/post/2016-12-11/nested.png)

ユニグラムの基底測度$G_0$は全語彙$V$の逆数の一様分布です。

この$G_0$をゼログラム分布と呼びます。

ユニグラム分布は$G_0$から生成され、バイグラム分布はユニグラム分布から生成され、トライグラム分布はバイグラムから生成されると仮定します。

具体的には以下のように生成されます。

$$
	\begin{align}
		p(\cdot) &\sim PY(\frac{1}{V}, d_0, \theta_0)\nonumber\\
		p(\cdot \mid the) &\sim PY(p(\cdot), d_1, \theta_1)\nonumber\\
		p(\cdot \mid the\ quick) &\sim PY(p(\cdot \mid the), d_2, \theta_2)\nonumber\\
		p(\cdot \mid the\ quick\ brown) &\sim PY(p(\cdot \mid the\ quick), d_3, \theta_3)\nonumber\\
	\end{align}\
$$

## 文脈木

階層Pitman-Yor過程の実装には文脈木を用います。

1つのPitman-Yor過程から生成された分布をレストランと呼び、文脈ごとにレストランが作られます。

たとえば以下のバイグラム分布はそれぞれ別のレストランにより管理されます。

$$
	\begin{align}
		p(\cdot \mid the) &\sim PY(p(\cdot), d_0, \theta_0)\nonumber\\
		p(\cdot \mid a) &\sim PY(p(\cdot), d_0, \theta_0)\nonumber\\
		p(\cdot \mid when) &\sim PY(p(\cdot), d_0, \theta_0)\nonumber\\
		p(\cdot \mid he) &\sim PY(p(\cdot), d_0, \theta_0)\nonumber\\
		p(\cdot \mid she) &\sim PY(p(\cdot), d_0, \theta_0)\nonumber\\
	\end{align}\
$$

集中度パラメータやディスカウント係数はnグラムオーダーごとに共通のものを使います。

NPYLMではこの文脈木に客を追加・削除することで客の配置をギブスサンプリングします。

文脈木の図の見方ですが、$the\ quick\ brown\ fox$という文があった場合、

トライグラムでは文脈として2単語を取りますので文脈は$quick\ brown$になり、客$fox$は文脈$quick\ brown$から生成されたと考えます。

4グラムでは文脈として3単語を取りますので文脈は$the\ quick\ brown$になり、客$fox$は文脈$the\ quick\ brown$から生成されたと考えます。

そのためレストラン$p(\cdot \mid the\ quick\ brown)$に客$fox$を追加しますが、文脈木はルートノードがユニグラムのレストランを表しています。

ルートノードの子ノードはバイグラム、その子ノードはトライグラム、その子ノードは4グラム、・・・

のように深く辿っていくほどnグラムのオーダーが増えます。

レストラン$p(\cdot \mid the\ quick\ brown)$にたどり着くためには、まずルートノードの子ノードの中から$brown$のノードを探します。

ノード$brown$が見つかれば次はその子ノードの中から$quick$のノードを探し、さらにその子ノードの中から$the$のノードを探します。

このノード$the$が、Pitman-Yor過程によって親の分布$p(\cdot \mid quick\ brown)$から生成された$p(\cdot \mid the\ quick\ brown)$になっています。

ではここで単語列$the\ quick\ brown\ fox$を観測したときに、レストラン$p(\cdot \mid the\ quick\ brown)$に単語$fox$を追加することを考えます。

![suffix_tree_1](/images/post/2016-12-11/suffix_tree_1.png)

Pitman-Yor過程において文脈$h$のレストランに単語$w$を追加する際は以下の確率に従って追加するテーブルを決定します。

- $max(0, c_{wk}-d)$に比例する確率で、単語$w$のテーブルの中から$k$番目のテーブルに客を追加
- $(\theta+dt_{\cdot})p(w \mid \pi(h))$に比例する確率で単語$w$のテーブルを新しく生成しそこに客を追加

$\pi(h)$はオーダーを一つ落とした文脈で、具体的には

$$
	\begin{align}
		h &= the\ quick\ brown\nonumber\\
		\pi(h) &= quick\ brown\nonumber\\
		\pi(\pi(h)) &= brown\nonumber\\
	\end{align}\
$$

のようになります。

ここではレストランにテーブル$fox$が1つもない場合を考えます。

![suffix_tree_2](/images/post/2016-12-11/suffix_tree_2.png)

この場合は$G_0 = p(\cdot \mid the\ quick)$からテーブル$fox$が生成され、客はそこに追加されます。

$G_0$からテーブルが生成された場合、代理客を親レストラン$G_0$（つまり$p(\cdot \mid the\ quick)$）に追加します。

これはスムージングのためと説明されていますが、Pitman-Yor過程では分布$G$は自らが生成した全ての客の配置に基づく経験分布と$G_0$によって決まるため、$p(\cdot \mid the\ quick)$が単語を生成したのなら$p(\cdot \mid the\ quick)$に客を追加して経験分布を更新するのは当然のことです。

この代理客の追加は再帰的に行われます。

つまり$p(\cdot \mid the\ quick)$に代理客を追加する際に、さらにその親$p(\cdot \mid the)$からテーブルが生成された場合、$p(\cdot \mid the)$にも代理客が追加されます。

その結果、ルートノードのレストラン（ユニグラム分布$p(\cdot)$）にはすべての単語のテーブルが存在します。

またレストランには同じ単語のテーブルが複数存在してもかまいません。

テーブルの数=親から生成された回数となります。

この文脈木を用いて文脈$h$に続く単語$w$の確率を以下のように計算します。

$$
	\begin{align}
		p(w \mid h) = \frac{c_{hw}-dt_{hw}}{\theta+c_{h\cdot}}+\frac{\theta+dt_{h\cdot}}{\theta+c_{h\cdot}}p(w|\pi(h))
	\end{align}\
$$

$c_{h\cdot}$はレストラン$h$の総客数、$c_{hw}$は単語$w$のテーブルに着席している総客数、$t_{hw}$は単語$w$の総テーブル数、$t_{\cdot}$は総テーブル数を表します。

式(9)の$p(w \mid \pi(h))$の部分は式(9)を再帰的に用いて計算され、$p(w \mid \pi(h))$がゼログラム分布に到達するまで計算を行ないます。

## HPYLMとVPYLM

式(9)が単語ユニグラムの場合、$p(w \mid \pi(h))$が単語ゼログラム確率になりますが、NPYLMではこれを一様分布ではなく文字nグラムモデルによって

$$
	\begin{align}
		G_0(w) &= p(c_1c_2...c_k)\\
		p(c_1c_2...c_k) &= p(c_1)p(c_2 \mid c_1)...p(c_k \mid c_1c_2...c_{k-1})
	\end{align}\
$$

のように計算します。

よってNPYLMは図のように単語nグラムモデルの基底測度に文字nグラムモデルが埋め込まれた階層nグラムモデルになります。

![npylm](/images/post/2016-12-11/npylm.png)

（論文より引用）

NPYLMにおいて単語nグラムの文脈木の深さはあまり必要ではありません。

そこでNPYLMの単語nグラムはバイグラムかトライグラムとし、深さは2か3に固定します。

よって固定の深さの階層ベイズ言語モデルであるHYPLMを単語nグラムに用います。

これは[以前のHPYLMの記事](/2016/07/26/A_Hierarchical_Bayesian_Language_Model_based_on_Pitman-Yor_Processes/)で解説を行っています。

一方で文字nグラムモデルはあらゆる文字列に適切な確率を与える必要があり、単語の文字列長は$1 \sim \infty$が考えられるため、オーダーnに依存するHPYLMでは問題が生じます。

そこで文字nグラムモデルには可変長nグラムモデルであるVPYLMを用います。

これも[以前のVPYLMの記事](/2016/07/28/Pitman-Yor%E9%81%8E%E7%A8%8B%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E5%8F%AF%E5%A4%89%E9%95%B7n-gram%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB/)で解説しているため参考にして下さい。

余談ですが[VPYLMとHPYLMのPythonラッパー](/2016/11/14/VPYLM-HPYLM%E3%81%AEPython%E3%83%A9%E3%83%83%E3%83%91%E3%83%BC/)もあります。

VPYLMの基底測度$G_0$は可能な総文字種類数の逆数の一様分布とします。

## 学習

NPYLMにおいて学習すべきパラメータはHPYLMとVPYLMを含めたすべての客の配置$\Theta$です。

$\Theta$の推定にはギブスサンプリングを用います。

これはHPYLMやVPYLMと同じで、観測された単語列をモデルに追加・削除することで客の配置をギブスサンプリングし最適化します。

ただし今回は単語境界が未知であるため、まず文を単語に分解する必要があります。

NPYLMでは文ごとの単語分割$\boldsymbol {\rm w}$を動的計画法によりまとめてサンプリングし、得られた単語列をHPYLMに渡し、得られた各単語の文字列をVPYLMに渡して学習を行います。

### HPYLM

HPYLMのパラメータ（全ての客の配置）$\Theta_{hpylm}$のギブスサンプリングでは、まず単語列$\boldsymbol {\rm w}=w_1w_2...w_n$のそれぞれの単語について、対応するレストランから客を一人削除します。（この時確率的に代理客も削除されます）

こうすると新しいパラメータ$\lnot\Theta_{hpylm}$が自動的に決まりますが、これによりパラメータがギブスサンプリングされたことになります。

その後それぞれの単語を再追加することで新たな客の配置$\Theta_{hpylm}$が決定します。（ギブスサンプリングされたことになります）

### VPYLM

HPYLMのユニグラムのノードにおいて、テーブル$w$が新たに生成された場合を考えます。

テーブルは単語ゼログラム分布（=VPYLM）から生成されるため、客$w$を文字列に分解した$\<bow\>c_1c_2...c_k\<eow\>$のそれぞれの文字をVPYLMに追加することによってパラメータ$\Theta_{vpylm}$を更新します。

この操作は階層Pitman-Yor過程においてテーブルが親から生成された際に代理客を親に送るのと同じ操作です。

ここでは代理客$w$の文字列$\<bow\>c_1c_2...c_k\<eow\>$が親（VPYLM）に送られています。

またVPYLMではHPYLMとは異なり、客を追加する際にオーダー$n$をサンプリングします。

文字列$\<bow\>c_1c_2...c_k\<eow\>$を追加する際にはオーダー$n_{bow}n_1n_2...n_kn_{eow}$がサンプリングにより決定し、このオーダーは文字列$\<bow\>c_1c_2...c_k\<eow\>$を削除する際に必要になるため保存しておく必要があります。

さらに同一の単語$w$のテーブルは複数存在しうるため、テーブルの数だけオーダー$n_{bow}n_1n_2...n_kn_{eow}$が存在することになります。

実装する際は$w$を提供している全てのテーブルの内、$m$番目のテーブルが生成されたときにVPYLMからサンプリングされたオーダー$n_{w_mbow}n_{w_m1}n_{w_m2}...n_{w_mk}n_{w_meow}$を取れるようにします。

![vpylm-hpylm](/images/post/2016-12-11/vpylm-hpylm.png)

図では\<bow\>と\<eow\>は省略しました。

一方でユニグラムのノードからテーブル$w$が消えた場合、VPYLMから$w$の文字列$\<bow\>c_1c_2...c_k\<eow\>$を削除することで新たな配置$\Theta_{vpylm}$をギブスサンプリングします。

## Forward filtering-Backward sampling

文$s$から単語列$\boldsymbol {\rm w}$をサンプリングするには、Forward filtering-Backward sampling法を用います。

### Forward filtering

HPYLMがバイグラムの場合は前向き確率$\alpha[t][k]$を用います。

注意点として、tはインデックスではなく番号ですので1から始まります。

これは$s$の部分文字列$c_1c_2...c_t$の最後の$k$文字が単語である確率を表しており、以下の再帰式により周辺化されています。

$$
	\begin{align}
		\alpha[t][k]=\sum_{j=1}^{t-k}p(c^t_{t-k+1} \mid c^{t-k}_{t-k-j+1})\cdot \alpha[t-k][j]
	\end{align}\
$$

ただし$c^m_n=c_n...c_m$です。

![forward-filtering-2](/images/post/2016-12-11/forward-filtering-2.png)

$p(X \mid Y)$はHPYLMにより計算します。

次にトライグラムの場合は$\alpha[t][k][j]$を用います。

これは$s$の部分文字列$c_1c_2...c_t$が、最後の$k$文字を単語とし、さらにその$j$文字前が単語である確率を表しています。

$$
	\begin{align}
		\alpha[t][k][j]=\sum_{i=1}^{t-k-j}p(c^t_{t-k+1} \mid c^{t-k-j}_{t-k-j-i+1}c^{t-k}_{t-k-j+1})\cdot \alpha[t-k][j][i]
	\end{align}\
$$

![forward-filtering-3](/images/post/2016-12-11/forward-filtering-3.png)

### Backward sampling

前向き確率テーブル$\alpha$が求まれば、文末から後ろ向きに可能な単語分割をサンプリングします。

文$s$の文字数を$N$とすると、$\alpha[N][k]$は最後の$k$文字が単語である確率を表します。

これに加えて文末には必ず文末を表す特殊な単語$$<eos>$$が存在するため、$$p(<eos> \mid c^N_{N-k+1})\cdot\alpha[N][k]$$に比例する確率で$k$をサンプリングします。

$k$をサンプリングしたら次は$\alpha[t-k][m]$に比例する確率で$m$をサンプリングし、次は$\alpha[t-k-m][n]$から$n$をサンプリングし、・・・

のように後ろから順に単語長をサンプリングし文$s$を単語に分割します。

トライグラムの場合、$\alpha[N][k][j]$は最後の$k$文字とその前の$j$文字が単語である確率を表します。

よって$$p(<eos> \mid c^{N-k}_{N-k-j+1}c^N_{N-k+1})\cdot\alpha[N][k][j]$$に比例する確率で$k$と$j$を同時にサンプリングします。

つまりトライグラムの場合は後ろから2単語を同時に分割します。

### 可能な単語の最大長

たとえば文$s$が100文字からなり、$t=100$の場合、$\alpha[100][1]$から$\alpha[100][100]$までを計算する必要があるのですが、実際の単語長はそこまで長くなく、おおよそ10～20文字が最大長であると考えられます。

そこでforward-filteringでは可能な単語の最大長を$L$とし、それ以上の長さの部分は計算を省略します。

つまり$\alpha[t][1]$から$\alpha[t][L]$までを計算し、$\alpha[t][L+1]$から$\alpha[t][t]$は計算しません。

式(12)の$\sum_{j=1}^{t-k}$も実際は$j=1$から$max(t-k, L)$までの総和をとります。

というよりもそうしないと$\alpha[t-k][j]$が未計算の部分にアクセスしてしまいます。

論文の図7で$for\ k=max(1, t-L)\ to\ t\ do$となっていますが、$\alpha[\cdot][k]$は$k=1$から$k=t-L$までが計算済みですので、式(12)の$\alpha[t-k][j]$の$j$はその計算済みの領域しかアクセスしてはいけません。

## ポアソン補正

文字nグラムモデルによる単語nグラムの基底測度の計算では、式(10)をそのまま使うと長い単語の確率が小さくなりすぎます。

式(11)を見れば明らかですが、単純に長い文字列は積の回数が増えるため短い文字列に比べてどうしても確率が小さくなってしまいます。

そこでNPYLMでは以下のようにポアソン分布を用いた補正を行います。

$$
	\begin{align}
		p(c_1...c_k) &= p(c_1...c_k \mid k, \Theta_{vpylm})p(k \mid \Theta_{vpylm})\\
		&= p(c_1...c_k, k \mid \Theta_{vpylm})\\
		&= \frac{p(c_1...c_k, k \mid \Theta_{vpylm})}{p(k \mid \Theta_{vpylm})}Po(k \mid \lambda)\\
	\end{align}\
$$

$p(k \mid \Theta_{vpylm}$は、VPYLMから長さ$k$の文字列（単語）が生成される確率を表しています。

また$\lambda$は以下の式からサンプリングし決定します。

$$
	\begin{align}
		p(\lambda \mid W) &\propto p(W \mid \lambda)p(\lambda)\\
		&= Ga(a+\sum_{w \in W}t(w)\mid w \mid, b+\sum_{w \in W}t(w))

	\end{align}\
$$

$t(w)$はHPYLMのルートノードにある$w$を提供しているテーブル数です。

$a$と$b$はハイパーパラメータですが、両方$1$にしても問題なく学習できます。

$\lambda$の推定はepochごとに行ないます。

## 実装

ここまでは論文に載っている部分の説明を行ないましたが、ここからは実際にコードを書く際に必要であり、かつ論文に載っていないことを中心に説明していきます。

### 文の扱い

すべての文には、文の始まりを表す特殊な文字\<bos\>（beginning-of-sentence）が文頭にあり、文の終わりには終端を表す特殊な文字\<eos\>（end-of-sentence）があります。

例）\<bos\> the quick brown fox jumps over the lazy dog \<eos\>

HPYLMがバイグラムの時は\<bos\>は文頭に１つだけ存在します。

トライグラムの時は\<bos\>が2つ存在します。

例）\<bos\> \<bos\> the quick brown fox jumps over the lazy dog \<eos\>

これは何故かと言うとHPYLMは固定オーダーなので、先頭のtheの確率を求めるときにトライグラムになるような文脈が必要だからです。

（HPYLMは確率を計算する際に必ず文脈が必要です）

### 単語の扱い

文と同様、すべての単語には単語の始まりを表す特殊な文字\<bow\>（beginning-of-word）が最初の文字として存在し、単語の終わりを表す特殊な文字\<eow\>（end-of-word）が単語の末尾に存在します。

例）\<bow\>quick\<eow\>

これは後述しますがVPYLMからの単語の生成において重要になります。

また単語（実際には文字列）をHPYLMに客として追加する際は、文字列を直接扱うのではなく数値で表される単語IDに変換します。

NPYLMでは文のあらゆる部分文字列が単語になり得るため、IDの管理は慎重に行う必要があります。

私は文をwstringで保持する実装をしているのですが、最初は部分文字列とIDの対応付けを行ない、未登録の文字列に新しいIDを割り当てる処理をしていました。

しかしNPYLMでは一度しか使われない部分文字列が大量に存在し無駄なIDを発行してしまうため、部分文字列のIDへの変換をC++のハッシュ関数hash\<wstring\>によってsize_t型のIDに変換するようにしました。

そのためハッシュが衝突しないことを祈りながらプログラムを動かしています。

このあたりの実装は私の経験のなさから最適な手法が思いつかなかったのですが、もっと良い手法があるかもしれません。

また部分文字列のwstringの各文字をVPYLMに追加する際の文字IDはwstringの各文字のwchar_t型の文字コードをそのまま使っています。

### レストラン

文脈木のノード（レストラン）のクラス構造です。

```
class Node{
	unordered_map<id, Node*> _children;		// 子ノード
	unordered_map<id, vector<int>> _arrangement;	// 客の配置 vector<int>のk番目の要素がテーブルkの客数を表す
	int _num_tables;	// 総テーブル数
	int _num_customers;	// 客の総数
	Node* _parent;		// 親ノード
	int _stop_count;	// 停止回数
	int _pass_count;	// 通過回数
	id _token_id;		// 単語ID　文字ID
	int _depth;		// ノードの深さ　rootが0
};
```

VPYLMの論文には子ノードや客の配置の管理にスプレー木を用いて高速化したとありますが、mapやunordered_mapでも十分な速度が出ます。

またboost::serializationによりデータの保存も容易に行うことができます。

### Forward filteringの特殊なケース

Forward filteringによる前向き確率$\alpha$の計算では特殊なケースが存在します。

まずバイグラムの場合ですが、$j$が存在しないケースを考えます。

これは$t=k$の時で、この場合$alpha[t][k]$は$p(c^t_{t-k+1} \mid \<bos\>)$で計算します。

![forward-filtering-2-special](/images/post/2016-12-11/forward-filtering-2-special.png)

次にトライグラムの場合は$i$が存在しないケースと$j$と$i$が両方存在しないケースが考えられます。

![forward-filtering-3](/images/post/2016-12-11/forward-filtering-3.png)
![forward-filtering-3-special](/images/post/2016-12-11/forward-filtering-3-special.png)

$i$が存在しないケースでは$\alpha[t][k][j]=p(c^t_{t-k+1} \mid \<bos\> c^{t-k}_{t-k-j+1})\alpha[t-k][j][0]$で計算します。

$i$も$j$も存在しないケースでは$\alpha[t][k][0] = p(c^t_{t-k+1} \mid \<bos\> \<bos\>)$のように計算します。

\<bos\>は他の文字と結合することがないため、このような特殊なケースとして考えると実装がしやすくなります。

（実際には\<bos\>は長さが0と考えますのでこのような特殊化をしなくても実装を行うことは可能です）

### ポアソン補正

単語確率のポアソン補正を行うには、VPYLMから長さ$k$の単語が生成される確率$p(k \mid \Theta_{vpylm})$を計算する必要があります。

この計算は論文にある通りランダムにVPYLMから単語を生成し、長さ$k$ごとに生成された回数をカウントし、生成されたすべての単語の総生成回数で割って正規化し確率にします。

単語の生成ではまず$p(\cdot \mid \<bow\>, \Theta_{vpylm})$から文字$c_1$をサンプリングします。

サンプリングのやり方ですが、全ての文字種について$p(c \mid \<bow\>, \Theta_{vpylm})$を計算し、その値に比例した確率で文字$c$を決定します。

次に$p(\cdot \mid \<bow\>c_1, \Theta_{vpylm})$から文字$c_2$をサンプリングします。

次に$p(\cdot \mid \<bow\>c_1c_2, \Theta_{vpylm})$から文字$c_3$をサンプリングします。

次に$p(\cdot \mid \<bow\>c_1c_2c_3, \Theta_{vpylm})$から文字$c_4$をサンプリングします。

これを\<eow\>が生成されるか可能な文字の最大長より少し多い$L+1$程度に到達するまでサンプリングを繰り返し単語を生成します。

およそ1000～5000個の単語を生成すれば充分に$p(k \mid \Theta_{vpylm})$を計算できます。

5000個の生成にかかる時間も数秒～十数秒程度です。

またVPYLMからサンプリングした単語長は短いものほど出現しやすい分布になりますので、ポアソン補正によって短い単語の確率にペナルティが与えられ、確率が小さな値に補正されます。

### ハイパーパラメータの推定

Pitman-Yor過程のハイパーパラメータ$d$と$\theta$は[A Bayesian Interpretation of Interpolated Kneser-Ney](https://www.stats.ox.ac.uk/~teh/research/compling/hpylm.pdf)の一番最後のページに載っている更新式を用いてサンプリングします。

この式は説明があまりされないまま出てきていますが、導出については[以前の記事](/2016/10/16/Pitman-Yor%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E3%83%8F%E3%82%A4%E3%83%91%E3%83%BC%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%81%AE%E6%8E%A8%E5%AE%9A/)をお読み下さい。

### ガンマ分布からのサンプリング

C++でガンマ分布からサンプリングを行う場合、\<random\>をincludeしてgamma_distribution\<double\>を使うと手軽に行なえます。

ただし注意点が一つあり、ガンマ分布は表記の仕方が2種類あり、計算方法がそれぞれ異なっています。

$$
	\begin{align}
		Ga(x \mid k, \theta) &= \frac{1}{\Gamma(k)\theta^k}x^{k-1}e^{-\frac{x}{\theta}}\\
		Ga(x \mid \alpha, \beta) &= \frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}\\
	\end{align}\
$$

C++のgamma_distributionは(19)式で実装されていますが、NPYLMに出てくるガンマ分布は全て(20)式で計算する必要があります。

式(20)を式(19)で計算する場合は$\theta$を逆数にすれば計算できるので、実装する際はご注意下さい。

### 親から生成されたと推定された回数

論文には頻繁に「親から生成されたと推定された回数」が出てきますが、これは単純にテーブルの数のことです。

たとえば単語$w$を提供しているテーブルが5個あった場合、単語$w$が親から生成されたと推定された回数は5になります。

## 実験

ここからはテキストデータを用いてNPYLMで教師なし形態素解析を行った実験結果を載せていきます。

以下、単語nグラムは全てトライグラムとします。

## Alice in Wonderland

不思議の国のアリスの原作です。

1135の文からなり、平均文長は99.19です。

Forward filtering-Backward samplingにおける可能な単語の最大長は$L=15$としました。

前処理として全ての空白文字と引用符" '、:;を除去し、大文字を全て小文字にしました。

ただしI'mやdidn'tの'は除去していません。

以下は各ギブスイテレーションにおける同一文の分割結果の移り変わりと推定された$\lambda$の値です。

```
Epoch 1 / 1000 - 2238.7 lps 0.000 ppl - 2679 nodes (vpylm) - 8 depth (vpylm) - 2249 nodes (hpylm)
alicewasbegin / ningtogetveryt / iredofsittingby / hersisterontheb / ank,andofhaving / nothingtodoonce / ortwiceshehad / peepedintothe / bookhersisterw / asreading,but / ithadnopictures / orconversatio / nsinit,andwhati / stheuseofabook, / thoughtalicew / ithoutp / ictures / orconver / sations? / 
lastly,shepic / turedtohersel / fhowthissamelit / tlesisterofhers / would,inthea / fter-time,beher / selfagrownwoman / 
andhowshewould / keep,throughall / herriperyears, / thesimpleandlov / ingheartofherch / ildhoodandho / wshewouldga / therabouthero / therlittlechi / ldren,andmaketh / eireyesbright / andeagerwithman / yastrangetale, / perhapsevenwith / thedreamofwonde / rlandoflongagoa / ndhowshewouldfe / elwithallth / eirsimplesorr / ows,andfinda / ple / asureinall / theirsimp / lejoys,r / ememberingherow / nchild-life,an / dt / hehapp / ysummerdays. / 
```

```
lambda <- 9.23032
Epoch 2 / 1000 - 26.4 lps 0.000 ppl - 2798 nodes (vpylm) - 8 depth (vpylm) - 21618 nodes (hpylm)
alice / was / beginn / ingto / get / very / tired / ofsittingby / hersister / onthe / bank, / andofhaving / nothing / todo / onceort / wice / shehad / peepe / d / into / thebook / hersister / wasreading / ,but / ithadnopictures / or / conversationsin / it / ,and / whatistheuseof / abook, / thought / alice / with / out / picturesorc / onversations? / 
last / ly,shepic / tured / tohersel / fhow / thissamelittles / isterofhers / would,inthe / after-time,be / herselfa / grownwoman / 
andhowshewould / keep,throughall / herriperyears, / thesimple / andlovinghea / rtofherchildho / od / and / howshewouldga / thera / boutherotherlit / tlechildren / ,and / maketheireyes / brightandeager / with / manyastranget / ale,perhapseven / with / thedreamofw / onderl / andoflongago / andhow / shew / ouldfeelwitha / llthe / irsimp / lesorrows / ,andfi / ndapleasure / ina / ll / theirs / implej / oys / ,re / me / mberingherownc / hild-li / fe, / and / thehap / pysummerdays. / 
```

```
lambda <- 4.57362
Epoch 5 / 1000 - 27.8 lps 0.000 ppl - 2187 nodes (vpylm) - 7 depth (vpylm) - 25426 nodes (hpylm)
alice / was / beginn / ingto / get / very / tired / of / sitting / by / hersister / onthe / bank, / and / ofhaving / nothing / todo / onceort / wice / shehad / peep / ed / into / the / book / hersister / wasreading / ,but / it / hadno / pictures / or / convers / ation / s / in / it / ,and / whatistheuseof / a / book / , / thought / alice / with / out / pictures / or / convers / ation / s / ? / 
last / ly / , / she / pic / tured / tohersel / fhow / this / same / little / sisterof / hers / would / , / in / the / after / - / time / , / be / herself / agrown / wo / man / 
and / how / she / would / keep / , / through / all / herripery / ears / , / thesimple / andloving / hear / t / of / her / child / hood / and / how / she / would / g / ather / about / her / other / little / children / ,and / make / theireyes / brigh / t / and / e / age / r / with / many / a / str / ange / tale / , / perhaps / even / with / the / dream / of / wonderl / and / of / longago / and / how / she / would / feel / with / all / the / irsimple / sorrow / s / ,and / find / a / ple / a / sure / in / all / their / simp / lejoys, / remember / ing / herown / child / - / life / ,and / the / happy / summerday / s. / 
```

```
lambda <- 4.23044
Epoch 10 / 1000 - 28.3 lps 0.000 ppl - 2731 nodes (vpylm) - 7 depth (vpylm) - 24689 nodes (hpylm)
alice / was / beginn / ing / to / get / very / tired / of / sitting / by / hersister / onthe / bank / ,and / of / having / nothing / todo / onceort / wice / shehad / peepe / d / into / the / book / hersister / wasreading / , / but / it / had / no / pictures / or / convers / ation / s / in / it / ,and / whatistheuseof / a / book / , / thought / alice / with / out / pictures / or / convers / ation / s / ? / 
last / ly / , / she / pic / tured / tohersel / f / how / this / same / little / sister / of / hers / would / , / in / the / after / - / time / , / be / herself / agrown / woman / 
and / how / she / would / keep / , / through / all / her / ripery / ears / , / thesimple / and / loving / hear / t / of / her / child / hood / and / how / she / would / gather / about / her / other / little / children / ,and / make / theireyes / brigh / t / and / e / age / r / with / many / a / str / ange / tale / , / perhaps / even / with / the / dream / of / wonderl / and / of / longago / and / how / she / would / feel / with / all / their / simp / le / sorrow / s / ,and / find / a / ple / a / sure / in / all / their / simp / le / joy / s / , / remember / ing / herown / child / - / life / ,and / the / happy / summerday / s / . / 
```

```
lambda <- 4.05505
Epoch 20 / 1000 - 28.1 lps 0.000 ppl - 3464 nodes (vpylm) - 7 depth (vpylm) - 24376 nodes (hpylm)
alice / was / beginn / ing / to / get / very / tired / of / sitting / by / hersister / on / the / bank / ,and / of / having / no / thing / todo / onceort / wice / shehad / peepe / d / into / the / book / hersister / was / read / ing / , / but / it / had / no / picture / s / or / convers / ation / s / in / it / ,and / whatistheuseof / abook / , / thought / alice / with / out / picture / s / or / convers / ation / s / ? / 
last / ly / , / she / picture / d / tohersel / f / how / this / same / little / sister / of / hers / would / ,inthe / after / - / time / , / be / herself / agrown / woman / 
and / how / she / would / keep / , / through / all / her / riperyears / , / thesimple / and / lov / ing / hear / t / of / her / child / hood / and / how / she / would / g / ather / about / her / other / little / children / , / and / make / their / eyes / brigh / t / ande / age / r / with / many / a / str / ange / tale / , / perhaps / even / with / the / dream / of / wonder / land / of / longago / and / how / she / would / feel / with / all / their / s / imple / sorrow / s / ,and / find / a / ple / a / sure / in / all / their / s / imple / joy / s, / remember / ing / her / own / child / - / life / , / and / the / happy / summerday / s / . / 
```

```
lambda <- 3.87907
Epoch 50 / 1000 - 27.1 lps 0.000 ppl - 4838 nodes (vpylm) - 7 depth (vpylm) - 24298 nodes (hpylm)
alice / was / beginn / ing / to / get / very / tired / of / sitting / by / hersister / on / the / bank / ,and / of / having / no / thing / todo / onceort / wice / she / had / peep / ed / into / the / book / hersister / was / read / ing / , / but / it / had / no / picture / s / or / convers / ation / s / in / it / , / and / what / is / theuseof / abook / , / thought / alice / with / out / picture / s / or / convers / ation / s / ? / 
last / ly / , / she / picture / d / tohersel / f / how / this / same / little / sister / of / hers / would / , / in / the / after / - / time / , / be / herself / a / grown / woman / 
and / how / she / would / keep / , / through / all / her / ri / pery / ears / , / the / simple / and / loving / heart / of / her / child / hood / and / how / she / would / ga / ther / about / her / other / little / children / , / and / make / their / eyes / bright / and / e / age / r / with / many / a / str / ange / tale, / perhaps / even / with / the / dream / of / wonder / land / of / longago / and / how / she / would / feel / with / all / their / simple / sorrow / s, / and / find / a / ple / a / sure / in / all / their / simple / jo / y / s / , / remember / ing / her / own / child / - / life / , / and / the / happy / summerday / s / . / 
```

```
lambda <- 3.91412
Epoch 100 / 1000 - 27.5 lps 0.000 ppl - 5628 nodes (vpylm) - 8 depth (vpylm) - 24632 nodes (hpylm)
alice / was / beginn / ing / to / get / very / tired / of / sitting / by / hersister / onthe / bank / ,and / of / having / no / thing / to / do / onceort / wice / she / had / peep / ed / into / the / book / hersister / was / read / ing / , / but / it / had / no / picture / s / or / convers / ation / s / in / it / ,and / what / is / theuseof / abook / , / thought / alice / with / out / picture / s / or / convers / ation / s / ? / 
last / ly / , / she / pictured / tohersel / f / how / this / same / little / sister / of / her / s / would / , / in / the / after / - / time / , / be / herself / a / grown / woman / 
and / how / she / would / keep / , / through / all / her / ri / pery / ears, / the / simple / and / loving / heart / of / her / child / hood / and / how / she / would / gat / her / about / her / other / little / children / ,and / make / their / eyes / bright / and / e / age / r / with / many / a / str / ange / tale / , / perhaps / even / with / the / dream / of / wonder / land / of / longago / and / how / she / would / feel / with / all / their / simple / sorrow / s / , / and / find / a / ple / a / sure / in / all / their / simple / joy / s, / remember / ing / her / own / child- / life / ,and / the / happy / summerday / s / . / 
```

```
lambda <- 3.80546
Epoch 200 / 1000 - 27.4 lps 0.000 ppl - 5972 nodes (vpylm) - 8 depth (vpylm) - 24884 nodes (hpylm)
alice / was / beginn / ing / to / get / very / tired / of / sitting / by / hersister / on / the / bank / ,and / of / having / no / thing / to / do / onceort / wice / she / had / peep / ed / into / the / book / hersister / was / read / ing / , / but / i / t / had / no / picture / s / or / convers / ation / s / in / it / , / and / what / is / theuseof / abook / , / thought / alice / with / out / picture / s / or / convers / ation / s / ? / 
last / ly / , / she / pictur / ed / tohersel / f / how / this / same / little / sister / of / her / s / would / , / inthe / after / - / time / , / be / herself / agrown / woman / 
and / how / she / would / keep / , / through / all / her / ri / per / year / s / , / the / simple / and / loving / heart / of / her / child / hood / and / how / she / would / g / at / her / about / her / other / little / children / ,and / make / their / eyes / bright / and / eage / r / with / many / a / str / ange / tale / , / perhaps / even / with / the / dream / of / wonder / land / of / longago / and / how / she / would / feel / with / all / their / simple / sorrow / s / , / and / find / a / plea / sure / in / all / their / simple / joy / s, / remember / ing / her / own / child / - / life / , / and / the / happy / summerday / s / . / 
```

パープレキシティの計算部分をコメントアウトしていた影響で0.000 pplになってますが無視して下さい。

lpsはlines per secondの略で、1秒間に何文のギブスサンプリングが行えるかを表しています。

次にランダムな50文をForward filtering-Backward samplingした時の結果です。

```
i / believe / so / , / alice / replied / thought / fully / . / they / have / the / irtailsinthe / irmouths / — / and / they / ’re / all / over / crumbs / . / 
then / the / queen / leftoff / , / quite / outofbreath / ,and / said / to / alice / , / haveyou / seen / the / mockturtle / yet / ? / 
either / the / well / was / very / deep / , / or / she / fell / very / slowly / , / for / she / had / plentyof / time / as / she / went / down / to / look / about / her / and / to / wonder / what / was / going / to / happe / nnext. / first / , / she / tried / to / look / down / and / make / out / what / she / was / coming / to / , / but / it / was / too / dark / to / see / any / thing / 
stupid / things / ! / alice / began / ina / loud / , / indignan / t / voice / , / but / she / stopped / hastily / , / for / the / w / hiterabbit / cried / out / , / silence / in / the / court / ! / and / theking / put / on / hisspec / tacl / es / and / looked / anxiously / round / , / to / make / out / who / was / talking / . / 
and / have / grown / most / un / common / ly / fat / 
if / i / 'm / mabel / , / i / 'll / staydownhere / ! / it'll / be / nouse / the / ir / put / ting / the / ir / heads / down / and / saying / “ / come / up / again / , / dear / ! / ” / 
i / t / did / so / indeed / ,and / much / so / on / er / than / she / had / expect / ed / before / she / had / d / runk / half / the / bo / ttle / , / she / found / her / head / press / ing / again / st / the / ce / iling / ,and / had / to / stoop / to / save / her / neck / from / being / broken / . / she / hastily / put / down / the / bo / ttle / , / saying / tohersel / f / that's / quite / enough / — / i / hope / i / shan't / grow / any / more / — / as / i / t / is / , / i / can't / get / out / at / the / do / or / — / i / do / wish / i / had / n't / drunk / quite / so / much / ! / 
down,down,down. / there / was / no / thing / else / to / do / , / so / alice / soon / began / talking / again / . / 
so / she / was / consider / ing / in / her / own / mind / as / well / as / shecould / , / for / the / hot / day / made / her / feel / very / sleepy / and / stupid / , / whether / the / ple / a / sure / of / making / a / da / is / y- / chain / wouldbe / worth / the / trou / ble / of / gettingupand / picking / the / da / is / ies, / when / sudd / enly / a / w / hiterabbit / with / pink / eyes / ran / close / by / her / . / 
why / didyou / call / him / t / or / toise / , / if / he / was / n't / one / ? / alice / asked / . / 
what / made / you / so / awfully / cl / ever / ? / 
there / 's / more / evidence / to / come / yet / , / ple / ase / yourmajesty / , / said / the / w / hiterabbit / , / jump / ing / up / inagreathurry / 
really / , / now / you / ask / me / , / said / alice / , / very / much / confus / ed / , / i / do / n't / think / — / 
i / said / pig / , / ’replied / alice / 
igaveherone,th / eygave / him / two / , / 
secondly,becaus / e / they / ’re / making / such / a / noise / in / side / , / no / one / could / po / ssibly / hear / you / . / and / certainly / there / was / a / most / extra / or / dina / ry / noise / going / on / with / in / — / a / constant / how / ling / and / sneez / ing / , / and / every / now / andthen / a / great / crash / , / as / if / a / dishorke / ttle / had / been / broken / to / pieces / . / 
there / was / no / thing / so / very / remark / able / in / that / 
i / t / matter / s / agoodde / al / to / me / , / said / alice / hastily / 
nor / did / alice / think / it / so / very / much / outof / the / way / to / hear / the / rabbit / say / to / itself / , / oh / dear / ! / oh / dear / ! / i / shall / be / late / ! / when / she / thought / it / over / after / wards / , / itoccurred / to / herthat / she / ought / to / have / wonder / ed / at / this / , / but / at / the / time / it / all / seem / edquitenatural / 
there / was / certainly / too / much / of / it / in / the / air / . / even / theduches / s / sneez / ed / occasion / ally / 
when / the / pie / was / all / finish / ed / , / theowl / , / as / abo / on / , / 
however / , / she / wenton / and / how / do / you / know / that / you / ’re / mad / ? / 
whocares / for / you / ? / said / alice / , / she / had / grown / to / her / full / size / by / this / time / . / you / ’re / no / thing / but / a / pa / ckof / card / s / ! / 
if / she / should / push / them / atter / on / , / 
they / were / learn / ingto / dra / w / , / the / dormouse / wenton / , / yawning / and / rubb / ing / its / eyes / , / for / i / t / was / ge / tting / very / sleepy / 
to / begin / with / , / said / thecat / , / adog / 's / not / mad / . / you / grant / that / ? / 
who / areyou / ? / said / the / caterpillar / . / 
poor / alice / ! / i / t / was / a / smu / chassheco / ulddo / , / lying / down / on / one / side / , / to / look / through / in / to / the / garden / withoneeye / 
ple / ase / yourmajesty / , / said / the / knave / , / i / did / n't / write / it, / and / they / can't / prove / i / did / there / 's / no / name / sign / ed / at / the / end / . / 
i / do / n't / believe / it / , / said / thepigeon / 
i / t / was / all / ridge / s / and / furrow / s / 
said / he / thank / ed / the / whiting / kindly / , / but / he / would / not / jo / inthedance. / 
well, / i / never / heard / it / before / , / said / themockturtl / e / 
we / must / burn / the / house / down / ! / said / therabbit / 's / voice / 
all / right / , / said / thecat / 
i / wish / i / had / our / dina / h / here / , / i / know / i / do / ! / said / alice / a / loud / , / ad / dress / ing / no / body / in / part / icular / . / she / 'd / soon / fetch / it / back / ! / 
not / the / same / thing / a / bit / ! / said / thehatter / . / you / might / just / as / well / say / that“i / see / what / i / eat / ” / is / thesameth / ing / as / “ / i / eat / what / i / see / ” / ! / 
a / large / ro / se-tree / stood / near / the / ent / rance / of / the / garden / the / rose / s / growing / onit / were / white / , / but / there / were / threegardeners / at / it / , / busily / painting / them / red / . / alice / thought / this / a / very / curious / thing / ,and / she / went / near / er / to / watch / the / m / , / and / just / as / she / came / up / to / the / m / she / heard / oneofthe / m / say / , / look / out / now / , / five / ! / do / n't / go / splash / ing / paint / over / me / like / that / ! / 
then / they / all / cro / wd / ed / round / her / once / more / , / while / the / do / do / solemnly / pres / ent / ed / the / thimble, / saying / we / beg / your / accept / anceof / thise / legant / thimble / ' / 
ofcourse / it / was / , / said / themockturtl / e / . / 
mary / an / n / ! / mary / an / n / ! / said / the / voice / . / fetch / me / my / gloves / this / moment / ! / ' / then / came / ali / ttlepatter / ingof / feet / on / the / stair / s / . / alice / knew / it / was / the / rabbit / coming / to / look / for / her / , / and / she / trembled / till / she / shook / the / house / ,quiteforgett / ing / that / she / was / now / about / a / thou / s / and / times / as / large / as / the / rabbit / ,and / had / no / reason / to / be / afraid / of / it / . / 
each / with / alobster / as / a / part / ner / ! / cried / thegryphon / . / 
and / anoldcrab / took / the / opportun / it / yof / saying / toher / daught / er / a / h / , / my / dear / ! / let / this / be / a / less / on / to / you / never / to / lose / your / temper / ! / ' / holdyour / tongue / , / ma / ! / said / the / young / crab / , / a / little / sn / appishly / . / you / ’re / enough / to / try / the / patience / ofa / noy / ster / ! / 
ple / ase / come / back / and / finish / your / story / ! / alice / call / ed / after / it / 
then / you / keep / moving / round / , / i / suppose / ? / said / alice / . / 
ten / hours / the / first / day / , / said / themockturtl / e / nine / the / next / ,and / soon / . / 
the / lobsters / ! / shouted / thegryphon / , / with / abound / in / to / the / air / . / 
what / can / all / that / green / stuff / be / ? / said / alice / . / and / where / have / my / shoulder / s / got / to / ? / and / oh, / my / poor / hand / s / , / how / is / it / i / can't / see / you / ? / she / was / moving / the / m / about / as / she / spoke / , / but / no / re / sult / seem / ed / to / follow / , / except / a / little / shaking / among / the / distant / greenleaves / . / 
al / so / its / eyes / were / ge / tting / extremely / small / for / a / baby / altogether / alice / did / not / like / the / look / of / the / thing / atall / . / but / perhaps / it / was / only / sobbing / , / she / thought / , / and / looked / in / to / its / eyes / again / , / to / see / if / there / were / any / tears / . / 
i / might / as / well / be / at / school / atonce / . / however / , / she / got / up / , / and / began / torepeatit / , / but / her / head / was / so / full / of / thelobster / quadrille / , / that / she / hardly / knew / what / she / was / saying / , / and / the / word / s / came / very / queer / indeed / — / 
```

教師なしでたった1,000文程度の英文ですが驚くほど綺麗に分割できています。

## 源氏物語

テキストデータは[源氏物語の世界](http://www.sainet.or.jp/~eshibuya/)からダウンロードしました。

全部で17,745行あり、平均文長は50.29です。

可能な単語の最大長は$L=10$としました。

前処理として全ての空白文字と「」『』を除去しました。

以下は各ギブスイテレーションにおける同一文の分割結果の移り変わりと推定された$\lambda$の値です。

```
Epoch 1 / 1000 - 2643.0 lps 0.000 ppl - 34426 nodes (vpylm) - 8 depth (vpylm) - 35403 nodes (hpylm)
いづれの御時にか、 / 女御、更衣あまた / さぶらひたまひけるな / かに、いとやむごと / なき際にはあらぬが、 / すぐれて時めきたまふ / ありけり。 / 
しばしは夢かとのみた / どられしを、やうやう / 思ひ静まるにしも、覚 / むべき方なく堪へがた / きは、いかにすべき / わざにかとも、問ひ / あはすべき / 人だになきを、忍びて / は参りたまひなむや。 / 
```

```
lambda <- 6.4899
Epoch 2 / 1000 - 119.7 lps 0.000 ppl - 30926 nodes (vpylm) - 8 depth (vpylm) - 252847 nodes (hpylm)
いづれ / の御時に / か、女御、更衣 / あまた / さぶらひたま / ひけるなかに、いと / やむごとなき / 際にはあらぬ / が、 / すぐれて / 時めきたまふ / ありけり。 / 
しばしは / 夢かとのみたどられし / を、やうやう思ひ静ま / るにしも、覚む / べき方なく堪へがた / きは、いかにすべき / わざにかとも、問ひ / あはすべき人 / だになきを、忍びては / 参りたまひなむや。 / 
```

```
lambda <- 3.71066
Epoch 5 / 1000 - 116.6 lps 0.000 ppl - 21611 nodes (vpylm) - 7 depth (vpylm) - 303368 nodes (hpylm)
いづれ / の / 御 / 時に / か、 / 女御、 / 更衣 / あまた / さぶらひたまひける / なかに、 / いと / やむごとなき / 際に / はあらぬ / が、 / すぐれて / 時めきたまふ / ありけり。 / 
しばしは / 夢かと / のみ / たどられ / しを、 / やうやう / 思ひ静まるにしも、覚 / むべき / 方なく / 堪へがたき / は、 / いかに / すべき / わざ / にか / とも、 / 問ひ / あはすべき人 / だになき / を、 / 忍びて / は / 参り / たまひなむ / や。 / 
```

```
lambda <- 3.08012
Epoch 10 / 1000 - 98.6 lps 0.000 ppl - 19888 nodes (vpylm) - 7 depth (vpylm) - 289781 nodes (hpylm)
いづれ / の / 御 / 時に / か / 、 / 女御、 / 更衣 / あまた / さぶらひたまひける / なかに、 / いと / やむごとなき / 際 / に / はあらぬ / が、 / すぐれて / 時めきたまふ / ありけり。 / 
しばしは / 夢かと / のみ / たどられ / し / を、 / やうやう / 思ひ / 静まる / にしも / 、 / 覚 / むべき / 方なく / 堪へがたき / は、 / いかに / すべき / わざ / に / か / とも、 / 問ひ / あはすべき / 人 / だになき / を、 / 忍びて / は / 参り / たまひなむ / や。 / 
```

```
lambda <- 2.5604
Epoch 20 / 1000 - 84.1 lps 0.000 ppl - 19537 nodes (vpylm) - 7 depth (vpylm) - 252990 nodes (hpylm)
いづれ / の / 御 / 時 / にか / 、 / 女御、 / 更衣 / あまた / さぶらひ / たまひ / ける / なかに、 / いと / やむごとなき / 際 / に / はあらぬ / が、 / すぐれて / 時めきたまふ / ありけり。 / 
しばし / は / 夢 / か / と / のみ / たどられ / し / を、 / やうやう / 思ひ / 静まる / にしも / 、 / 覚む / べき / 方 / な / く / 堪へがたき / は、 / いかに / すべき / わざ / にか / と / も / 、 / 問ひ / あはすべき / 人 / だになき / を / 、 / 忍びて / は / 参り / たまひ / なむ / や。 / 
```

```
lambda <- 2.17746
Epoch 50 / 1000 - 76.3 lps 0.000 ppl - 18469 nodes (vpylm) - 7 depth (vpylm) - 216751 nodes (hpylm)
いづれ / の / 御 / 時 / に / か / 、 / 女御 / 、 / 更衣 / あまた / さぶらひ / たまひ / ける / なかに / 、 / いと / やむごと / な / き / 際 / に / は / あ / ら / ぬ / が / 、 / すぐれ / て / 時めきたまふ / あり / けり / 。 / 
しばし / は / 夢 / か / と / のみ / たどられ / し / を / 、 / やうやう / 思ひ / 静まる / にしも / 、 / 覚む / べ / き / 方 / な / く / 堪へがたき / は / 、 / いかに / す / べ / き / わざ / に / か / と / も / 、 / 問ひ / あはすべき / 人 / だに / な / き / を / 、 / 忍びて / は / 参り / たまひ / なむ / や。 / 
```

```
lambda <- 2.01836
Epoch 100 / 1000 - 83.0 lps 0.000 ppl - 17297 nodes (vpylm) - 7 depth (vpylm) - 201210 nodes (hpylm)
いづれ / の / 御 / 時 / に / か / 、 / 女御 / 、 / 更衣 / あまた / さぶらひ / たまひ / ける / なかに / 、 / いと / やむごと / な / き / 際 / に / は / あ / ら / ぬ / が / 、 / すぐれ / て / 時めき / たまふ / あり / けり / 。 / 
しばし / は / 夢 / か / と / のみ / たどられ / し / を / 、 / やうやう / 思ひ / 静まる / にしも / 、 / 覚むべき / 方 / な / く / 堪へがた / き / は / 、 / いかに / す / べ / き / わざ / に / か / と / も / 、 / 問ひ / あはすべき / 人 / だに / な / き / を / 、 / 忍び / て / は / 参り / たまひ / なむ / や。 / 
```

次にランダムな50文をForward filtering-Backward samplingした時の結果です。

```
宮 / は / 内裏に / とまり / たまひ / ぬ / る / を / 見 / おき / て、 / ただ / なら / ず / おはし / た / る / なめり / 。 / 
おりたちて / 汲み / は / 見 / ねど / も / 渡り / 川 / 
木工の君 / は / 、 / 殿 / の / 御 / 方 / の / 人 / に / て / とどまる / に、 / 中将の御許 / 、 / 浅 / けれど / 石間の水は / 澄み / 果て / て / 
ねび / 人 / ども / は / 、 / いと / あやし / く / 心得 / がた / く / 思ひ / 惑は / れ / けれど / 、 / さりとも / 悪し / ざま / なる / 御 / 心 / あ / らむ / や / は / と / 慰め / た / り。 / 
消え / ぬ / に / かかる / 花と見る見る / 
初めより / おし / なべての / 上 / 宮仕へ / し / たまふ / べ / き / 際 / に / は / あ / ら / ざり / き / 。 / 
昔 / の / 恋し / き / 御 / 形見に / は / 、 / この / 宮 / ばかり / こそ / は / 。 / 
人まに / さし / 寄り / て / 、 / も / の / 隠し / は / 懲り / ぬ / らむ / かし / と / て / 、 / いと / ねた / げ / なる / しり / 目 / なり。 / 
右の / 大殿 / の / 御 / 子 / ども / 二人 / 、 / 大将 / の / 御 / 子 / 、典侍の / 腹 / の / 加へて / 三人 / 、 / まだ / 小さき / 七 / つ / より / 上の / は / 、 / 皆 / 殿 / 上 / せさせ / たまふ。 / 
かかる / 命 / のほど / を / 知らで / 、 / 行く末 / 長 / く / のみ / 思ひ / はべり / ける / こと / と / 、 / 泣く泣く / 渡り / たまひ / ぬ / 。 / 
御 / 座 / を / 譲り / たまへ / る / 仏 / の / 御 / しつらひ / 、 / 見 / やり / たまふ / も / 、 / さまざま / に / 、 / かかる / 方 / の / 御 / いとなみ / を / も / 、 / もろともに / 急が / む / もの / と / は / 思ひ / 寄ら / ざり / し / こと / なり / 。 / 
いでや / 、 / ものげな / しと / あなづり / きこえ / させ / たまふ / に / はべる / めり / かし / 。 / 
あな / 心憂 / と / て / 、 / 異事に言ひ / 紛らは / し / たまひ / つ / 。 / 
僧都 / に / 会ひ / て / こそ / は / 、 / たしかなる / ありさま / も / 聞き / 合はせ / などし / て / 、 / ともかくも / 問ふ / べ / か / めれ / など / 、 / ただ / 、 / この / こと / を / 起き / 臥し / 思 / す / 。 / 
手 / を / 書き / た / る / に / も、 / 深 / き / こと / は / な / く / て / 、 / ここ / かしこ / の / 、 / 点 / 長 / に / 走り / 書き / 、 / そこはかとなく / 気色 / ばめる / は / 、 / うち / 見 / る / に / かどかどし / く / 気色 / だち / た / れど / 、 / なほ / ま / ことの / 筋 / を / こまやかに / 書き / 得 / た / る / は / 、 / うはべ / の / 筆 / 消え / て / 見ゆ / れど / 、 / 今 / ひと / たび / とり / 並べ / て / 見 / れば / 、 / なほ / 実 / に / なむ / より / ける / 。 / 
え / 引き / 隠し / たま / はで / 、 / 御覧ず。 / 
故 / 君 / の / 常 / に / 弾きたまひし / 琴 / なりけり / 。 / 
ほのかに / も / 御 / 声をだに / 聞か / ぬ / こと / など / 、 / 心 / に / も / 離れ / ず / 思ひ / わた / り / つ / る / ものを / 、 / 声 / は / つひに / 聞か / せ / たま / は / ず / なり / ぬ / る / にこそはあめれ / 、 / むなし / き / 御 / 骸 / に / て / も / 、 / 今一度 / 見たてま / つらむ / の / 心ざし / かなふ / べ / き / 折 / は / 、 / ただ / 今 / よりほかに / いかでか / あらむ / と / 思ふ / に / 、 / つつみもあへ / ず / 泣かれ / て / 、 / 女房 / の / 、 / ある / 限り / 騷ぎ / 惑ふ / を / 、 / あなかま / 、 / しばし / と / 、 / しづめ / 顔にて / 、 / 御 / 几帳の帷を / 、 / も / の / のたまふ / 紛れに / 、 / 引き上げ / て / 見 / たまへ / ば / 、 / ほのぼの / と / 明けゆく / 光 / も / おぼつかな / ければ、 / 大殿油 / を / 近く / かかげ / て見たてまつ / りたまふに、 / 飽かず / うつく / し / げ / に / 、 / めでた / う / きよら / に / 見ゆ / る / 御 / 顔 / の / あたら / し / さ / に / 、 / この / 君 / の / かく / のぞき / たまふ / を見る見るも、 / あながちに / 隠 / さ / む / の御心 / も / 思 / さ / れ / ぬ / なめり / 。 / 
ま / ことの / 心ばへ / の / あはれ / なる / を / 見 / ず / 知ら / ず / は / 、 / かう / まで / 思ひ / 過ぐ / す / べ / く / も / な / き / け / 疎 / さ / かな / と / 、 / 思ひ / ゐ / たまへり / 。 / 
あやし / う / 、 / いと / こよな / く / およすけ / たまへ / り / し / 人 / の / 、 / かかる / べ / う / て / や / 、 / この / 二、 / 三年 / の / こなた / なむ / 、 / いたう / しめり / て / 、 / も / の / 心細 / げ / に / 見え / たまひ / し / かば / 、 / あまり / 世 / の / ことわり / を / 思ひ / 知り / 、 / も / の / 深 / う / なり / ぬ / る / 人 / の / 、 / 澄み / 過ぎ / て / 、 / かかる / 例 / 、 / 心 / うつく / し / か / ら / ず / 、 / かへりて / は / 、 / あざやかなる / 方 / の / おぼえ / 薄らぐ / もの / なり / と / なむ / 、 / 常に / はかばか / しからぬ / 心 / に / 諌め / きこえ / し / かば / 、 / 心 / 浅 / し / と / 思ひ / たまへ / り / し / 。 / 
いふかひ / な / く / あはれ / に / て / 、 / それ / は / 、 / 老い / て / はべれ / ば / 醜 / き / ぞ / 。 / 
涙ぐみ / て / 聞き / おはす。 / 
いとほし / か / り / し / もの / 懲り / に / 、 / 上げ / も / 果て / たま / はで / 、 / 脇息 / を / おし / 寄せ / て / 、 / うち / かけ / て / 、 / 御 / 鬢ぐき / の / しどけなき / を / つくろひ / たまふ / 。 / 
何ごとも / 、 / いと / つきな / か / ら / む / は / 口惜し / か / らむ / 。 / 
大臣 / 渡り / たまひ / て / 、 / 一 / 日 / の / 興 / あり / し / こと / 、 / 聞こえ / たまふ / 。 / 
年ごろ / ならひ / たま / はぬ / 旅 / 住み / に / 、 / 狭く / はしたな / く / て / は / 、 / いかでか / あまた / は / さぶらは / む / 。 / 
大将 / 、 / 参り / たまへり。 / 
おはせ / ぬ / よし / を / 言へど、 / 昼の使 / の / 、 / 一 / 所 / など / 問ひ / 聞き / た / る / なるべし / 、 / いと / 言 / 多く / 怨み / て / 、 / 御 / 声 / も / 聞き / はべら / じ / 。 / 
一条の / 大路 / 、 / 所 / な / く / 、 / むくつけ / き / まで / 騒ぎ / たり。 / 
つれなく / て / 今宵の明け / つ / らむ / と / 、 / 言ふ / べ / き / 方 / の / な / けれ / ば / 、 / 女 / 君 / ぞ / 、 / いと / つら / う / 心憂 / き / 。 / 
昔 / 、 / 大将 / の / 御 / 母 / 亡せ / たまへ / り / し / も / 、 / この / ころ / の / こと / ぞ / かし / と / 思 / し / 出づ / る / に / 、 / いと / も / の / 悲し / く / 、 / そ / の / 折 / 、 / かの / 御 / 身 / を / 惜し / み / きこえ / たまひ / し / 人 / の / 、 / 多く / も / 亡せ / たまひ / に / ける / かな / 。 / 
わが / 身 / は / 、 / とてもかくても / 同じ / こと / 。 / 
こなた / に / と / て / 、 / 御 / 几帳 / 隔て / て / 入れ / たてま / つり / たまへり / 。 / 
昨日 / おはし / 着き / なむ / と / 待ち / きこえ / させ / し / を / 、 / などか / 、 / 今日 / も / 日たけて / は / と / 言ふ / めれ / ば / 、 / こ / の / 老い / 人 / 、 / いと / あやし / く / 苦し / げ / に / の / み / せさせ / たまへ / ば / 、 / 昨日 / は / この / 泉 / 川の / わたり / に / て / 、 / 今 / 朝 / も / 無期に / 御心地 / ためらひて / なむ / と / いらひて、 / 起こ / せ / ば / 、 / 今 / ぞ / 起き / ゐ / た / る / 。 / 
おりたちて / あながち / なる / 御 / もてなし / に / 、 / 女 / は / さ / も / こそ負け / たてま / つら / め / 。 / 
朝ぼらけ / の / 空 / に / 雁 / 連れて / 渡 / る / 。 / 
はかなき / 一 / 節 / に / 、 / かう / は / もてな / し / たまふ / べ / く / や / と / 、 / いみじ / う / あはめ / 恨み / 申 / し / たまへば、 / 何ごと / も / 、 / 今は / と / 見 / 飽き / たまひ / に / ける / 身 / なれば / 、 / 今 / は / た / 、 / 直 / る / べ / き / にもあらぬを、 / 何か / は / と / て / 。 / 
も / し / 、 / すか / いたまふ / か / と / も / 言ひ / あへ / ず / 、 / 人 / 々 / 起き騒ぎ / 、 / 上 / の / 御 / 局 / に / 参 / り / ちがふ / けしき / ども / 、 / しげく / まよ / へば、 / いと / わりな / く / て / 、 / 扇 / ばかり / を / しるし / に / 取り / 換へ / て、 / 出でたまひぬ。 / 
たびたび / そそのかし / たまへど、 / とかく / 聞こえ / すさび / て / 、 / やみ / たまひ / ぬ / めれ / ば / 、 / いと / 口惜し / う / おぼゆ / 。 / 
いかに / 思 / し / つ / らむ / など / 、 / よろづに / 思ひ / ゐ / たまへ / る / ほど / に / 、 / 御文 / あり。 / 
朱雀院の / 帝 / 、 / ありし / 御幸 / の / のち / 、 / そ / の / ころほひ / より / 、 / 例 / なら / ず / 悩み / わたらせ / たまふ / 。 / 
いと / 尊 / く / おはせ / し / あたり / に / 、 / 仏 / を / しるべにて / 、 / 後の世 / を / の / み / 契り / し / に / 、 / 心 / きたな / き / 末 / の / 違ひ / め / に / 、 / 思ひ / 知ら / す / る / なめり / と / ぞ / おぼゆ / る / 。 / 
いと / ささやか / なる / 人 / の / 、 / 常 / の / 御 / 悩み / に / 痩せ / 衰へ / 、 / ひはづ / に / て / 、 / 髪 / いと / けうらに / て / 長 / か / り / ける / が / 、 / わけ / た / る / やう / に / 落ち細り / て / 、 / 削ること / も / をさをさ / し / たま / は / ず / 、 / 涙 / に / まつはれ / た / る / は / 、 / いと / あはれ / なり。 / 
裳 / は / 、 / ただ / 今 / 我 / より / 上 / なる / 人 / な / き / に / うち / たゆみ / て / 、 / 色 / も / 変へ / ざり / けれ / ば / 、 / 薄色 / なる / を / 持 / た / せ / て / 参 / る / 。 / 
峰 / の / 雪 / みぎは / の / 氷 / 踏み / 分け / て / 
西面 / に / は / 、 / わざと / な / く / 、 / 忍びやかに / うち / 振る舞ひ / たまひ / て / 、 / 覗き / たまへ / る / も / 、 / めづら / しき / に / 添へ / て / 、 / 世 / に / 目 / なれ / ぬ / 御 / さま / なれば / 、 / つら / さ / も / 忘れ / ぬ / べ / し。 / 
心 / 知り / の / 人 / 二人 / ばかり / 、 / 心 / を / 惑は / す / 。 / 
いらへ / たまふ / べ / く / も / あ / ら / ねば / 、 / 尼君 / 、 / 待 / 乳 / の / 山 / 、 / と / なむ / 見たまふる / と / 言ひ / 出だし / たまふ / 。 / 
かかる / 御 / 住まひ / は / 、 / すずろなる / こと / も / 、 / あはれ / 知る / こそ / 世の常 / の / こと / なれ / など / 、 / こしらへ / て / も / 言へど、 / 人 / に / もの / 聞こゆ / らむ / 方 / も / 知らず / 、 / 何事も / いふかひ / な / く / のみ / こそ / と / 、 / いと / つれなく / て / 臥し / たまへり / 。 / 
さ / て / も / 、 / いと / うつく / し / かり / つ / る / 児 / かな / 。 / 
```

やや切り過ぎな場所もありますがおおよそ自然な分割になっていると思います。

## 実行速度

[持橋先生のスライド](http://chasen.org/~daiti-m/paper/Robotics-20130327.pdf)には1秒間に100～200文の解析が行えると書かれてありました。

1文を使ったパラメータのギブスサンプリングにかかる時間は文長$N$と可能な単語の最大長$L$に影響されるのですが、私の実装では平均100文字、$L=15$のAlice in Wonderlandで1秒27文、平均50文字、$L=10$の源氏物語で1秒80文でした。

## おわりに

NPYLMはベイズ的な階層nグラムモデルとして自然なものであり、動作を視覚的にイメージしやすいということもあって理論自体は比較的易しいものになっています。

しかし実際にコードに落とし込むとなるとデバッグに多大な時間を費やすことになり、また論文に書かれていない細かい部分をどのように実装すべきかの指針がないため非常に苦労しました。

NPYLMは一度で完璧なものを作るのではなく、何回か書き直すごとに洗練された効率の良いコードになっていくものだと思います。

今後はPythonで使えるラッパーの開発や、NPYLMにもとづく教師なしトレンドワード解析やword2vecによる単語ベクトルの教師なし学習などをやってみようと思います。

またNPYLMベースで品詞まで学習する教師なし完全形態素解析や半教師あり学習との協調学習などの論文もありますので今後実装していきたいと思います。