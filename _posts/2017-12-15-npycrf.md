---
layout: post
title:  条件付確率場とベイズ階層言語モデルの統合による半教師あり形態素解析（NPYCRF）
category: 実装
tags:
- NPYCRF
excerpt_separator: <!--more-->
---

この記事は[自然言語処理 Advent Calendar 2017](https://qiita.com/advent-calendar/2017/nlp)の19日目の記事です。

<!--more-->

## はじめに

今回は半教師あり形態素解析の手法であるNPYCRFについて、その理論と実装方法をまとめました。

一般に形態素解析はCRFによる識別モデルが用いられ、CRFのパラメータは教師付きデータにより最適化されます。

しかし、古くはしょこたんブログに始まり、最近ではTwitterなどのソーシャルメディアで日々新しい単語が生み出されているため、こうしたテキストデータを適切に解析できるモデルが求められます。


（余談ですが最近は単語を省略するのが流行っているらしく、例えば「バイト」を「バ」と言うそうです。例）「バイト終わった」→「バおわ」）


NPYCRFは、教師なしデータの単語分割のための言語モデルであるNPYLMと、形態素解析のための識別モデルであるCRFを統合し、教師付きデータの分割基準を守りながら教師なしデータの単語分割を学習できるようになっています。

NPYLMに加えてCRFとL-BFGSを実装しなければならないため、実装難易度は当ブログ史上最高となっています。

最初から実装する場合は半年、NPYLMをベースに実装する場合は2ヶ月程度かかると思います。

また、通常こういった論文はページ数の制約があるため、本当にすべきことの1割程度しか論文に書かれません。

この記事では私の解釈に基づいて残りの9割の実装方法を説明するため、誤りを含む可能性があります。ご了承ください。

## 参考文献

実装するにあたり必要な文献です。

- [半教師あり形態素解析NPYCRFの修正](http://www.anlp.jp/proceedings/annual_meeting/2016/pdf_dir/D6-3.pdf)
	- オリジナル版のNPYCRFにはモデル統合部分に誤りがあります。
- [Nonparametric Bayesian Semi-supervised Word Segmentation](http://chasen.org/~daiti-m/paper/tacl2016semiseg.pdf)
	- 修正版の国際学会版です。より詳しくなっています。
- [条件付き確率場の理論と実践](http://www.ism.ac.jp/editsec/toukei/pdf/64-2-179.pdf)
	- CRFの理論や勾配計算について非常に詳しくまとまっています。
- [ベイズ階層言語モデルによる教師なし形態素解析](http://chasen.org/~daiti-m/paper/nl190segment.pdf)
	- NPYLMの論文です。
	- 去年[解説記事](/2016/12/14/%E3%83%99%E3%82%A4%E3%82%BA%E9%9A%8E%E5%B1%A4%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AB%E3%82%88%E3%82%8B%E6%95%99%E5%B8%AB%E3%81%AA%E3%81%97%E5%BD%A2%E6%85%8B%E7%B4%A0%E8%A7%A3%E6%9E%90/)を書きました。

## 実装

コア実装をC++14で行い、BoostでPythonから利用できるようにしています。

[https://github.com/musyoku/python-npycrf](https://github.com/musyoku/python-npycrf)

まだ細かい部分が未完成ですが学習と単語分割はできます。

またNPYLMは特許が取られているため、この実装は研究以外の用途に使用しないでください。

<blockquote class="twitter-tweet" data-lang="ja"><p lang="ja" dir="ltr">正しい連絡先が分からないのでここで：「ご注文は機械学習ですか?」で特許のため公開できないとされているNPYLMのGitHub上の実装は、NTT知財に確認してもらったところ「試験または研究のための使用」であれば公開してもよいそうです。他の方の実装も、基本的に同じだと思います。</p>&mdash; Daichi Mochihashi (@daiti_m) <a href="https://twitter.com/daiti_m/status/851810748263157760?ref_src=twsrc%5Etfw">2017年4月11日</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


## NPYLMとCRF

NPYLMはセミマルコフモデル、CRFはマルコフモデルになっており、単語分割は以下のように表されます。

NPYLMは各ノードが単語を表しています。

CRFは1（＝語頭）と0（＝それ以外）の2つラベルを持ちます。

## 識別-生成統合モデル

NPYCRFではJESS-CM法を用いて、入力文$\boldsymbol x = x_1x_2 \cdots x_n$に対するラベル列$\boldsymbol y$の確率を以下のように表現します。

$$
	\begin{align}
		P_{\text{CONC}}(\boldsymbol y \mid \boldsymbol x) \propto P_{\text{CRF}}(\boldsymbol y \mid \boldsymbol x;\boldsymbol \Lambda)P_{\text{NPYLM}}(\boldsymbol y, \boldsymbol x;\boldsymbol \Theta)^{\lambda_0}
	\end{align}\
$$

$P_{\text{CRF}}$はCRF（識別モデル）、$P_{\text{NPYLM}}$はNPYLM（生成モデル)であり、$\boldsymbol \Lambda=(\lambda_1, \lambda_2, ..., \lambda_K)$はCRFのパラメータ（重み）、$\boldsymbol \Theta$はNPYLMのパラメータ（中華料理店過程の客の配置）を表しています。

$\propto$は比例を意味しているので式(1)は確率ではありませんが、$\boldsymbol x$に対する可能な全ての分割$\boldsymbol y$で式(1)を計算した総和$Z(x)^*$で割れば確率になるため、式(1)を等号で表すことができます。

ここで$\text{log}P_{\text{NPYLM}}(\boldsymbol y, \boldsymbol x;\boldsymbol \Theta)$を連続値を返す素性関数とみなせば、式(1)はパラメータ$\boldsymbol \Lambda^*=\lambda_0, \lambda_1, ..., \lambda_K$を持つ対数線形モデルの形で書くことができます。

$$
	\begin{align}
		P_{\text{CONC}}(\boldsymbol y \mid \boldsymbol x) &\propto \text{exp}
			\left(
					\lambda_0 \text{log}P_{\text{NPYLM}}(\boldsymbol y, \boldsymbol x;\boldsymbol \Theta)
					+
					\sum_{k=1}^K \lambda_k f_k(\boldsymbol y, \boldsymbol x)
			\right)\\
		&= \text{exp}(\boldsymbol \Lambda^* F^*(\boldsymbol y, \boldsymbol x))
	\end{align}\
$$

$f_k$はCRFの$k$番目の素性関数です。

## モデル変換

JESS-CMによるモデル統合では、識別モデルと生成モデルが同じ構造をしている必要があります。

しかしNPYCRFでは識別モデルがマルコフモデルであるのに対し、生成モデルがセミマルコフモデルになっているため、各モデルの情報を直接統合することができないので、何らかの変換処理を行う必要があります。

この変換処理はオリジナル版のNPYCRFに誤りがあり、修正版で正しい統合方法が提案されています。

## オリジナル版のモデル変換

ここからはNPYLMが単語bigramモデルであるとします。

文$\boldsymbol x$の$s$番目の文字$x_s$から$t$番目の文字$x_t$までの部分文字列を$c_s^t$で表します。

この時、マルコフモデル上の$c_s^t$に対応する区間$[s, t)$のポテンシャルを$\gamma(s, t)$とおくと、これは状態1で始まり状態1で終わるV字型の区間になります。

セミマルコフモデルの各ノードの遷移確率$P(c_t^{u-1} \mid c_s^{t-1})$に対応するポテンシャルとして、オリジナル版のNPYCRFでは

$$
	\begin{align}
		P(c_t^{u-1} \mid c_s^{t-1}) &\propto \text{exp}\left(\gamma(s, t) + \gamma(t, u)\right)
	\end{align}\
$$

を用いてNPYLMの前向き確率を

$$
	\begin{align}
		\alpha[t][k] = \sum_{j=1}^{t-k} \text{exp} 
			\left\{ 
				\lambda_0 \text{log}\left( P(c_{t-k+1}^t \mid c_{t-k-j+1}^{t-k}) \right) 
				+ \gamma(t-k-j+1, t-k+1) + \gamma(t-k+1, t+1)
			\right\} \cdot \alpha[t-k][j]
	\end{align}\
$$

としています。

$\alpha[t][k]$は、位置$t$から左の最も近い単語境界までの距離が$k$文字である確率を表しており、それ以前の分割全てについて周辺化された確率になっています。

マルコフモデルの前向き確率のようなものだと考えてください。

式(5)の$\text{exp}(\cdot)$が、CRFの情報を統合したNPYLMのセミマルコフモデル上での遷移確率になります。

オリジナル版のNPYCRFの誤りを分かりやすくするため、ここでは可能な単語の最大長を1として、長さ3文字の文$\boldsymbol x = x_1x_2x_3$の前向き確率を求めてみます。

（文頭に\<bos\>、文末に\<eos\>を挿入しています）

単語の最大長が1なので周辺化の処理がいらなくなり、以下のように前向き確率を求められます。

$$
	\begin{align}
		\alpha[1][1] &= \text{exp} \left\{ \lambda_0 \text{log}\left( P(c_1^1 \mid \text{<bos>}) \right) 
						+ \gamma(0, 1) + \gamma(1, 2) \right\}  \nonumber \\
		\alpha[2][1] &= \text{exp} \left\{ \lambda_0 \text{log}\left( P(c_2^2 \mid c_1^1) \right) 
						+ \gamma(1, 2) + \gamma(2, 3) \right\} \cdot  \alpha[1][1] \nonumber \\
		\alpha[3][1] &= \text{exp} \left\{ \lambda_0 \text{log}\left( P(c_3^3 \mid c_2^2) \right) 
						+ \gamma(2, 3) + \gamma(3, 4) \right\} \cdot  \alpha[2][1] \nonumber \\
		\alpha[4][1] &= \text{exp} \left\{ \lambda_0 \text{log}\left( P(<eos> \mid c_3^3) \right) 
						+ \gamma(3, 4) \right\} \cdot  \alpha[3][1]\\
	\end{align}\
$$

前向き確率の一番最後の値$\alpha[4][1]$は規格化定数$Z(x)^*$になります。

またこのケースでは可能な分割が1通りしか存在しないため、

$$
	\begin{align}
		P_{\text{CONC}}(\boldsymbol y \mid \boldsymbol x) &= P_{\text{CRF}}(\boldsymbol y \mid \boldsymbol x;\boldsymbol \Lambda)P_{\text{NPYLM}}(\boldsymbol y, \boldsymbol x;\boldsymbol \Theta)^{\lambda_0} \nonumber \\
		&= Z(x)^* \nonumber \\
		&= \alpha[4][1]
	\end{align}\
$$

となります。

式(6)と式(7)から以下のようになります。

$$
	\begin{align}
		P_{\text{CONC}}(\boldsymbol y \mid \boldsymbol x) &= \alpha[4][1] \nonumber \\
		&= \text{exp} \left\{ \lambda_0 \text{log}\left( P(<eos> \mid c_3^3) \right) 
						+ \gamma(3, 4) \right\} \cdot  \alpha[3][1] \nonumber \\
		&= \text{exp} \left\{ \lambda_0 \text{log}\left( P(<eos> \mid c_3^3) \right) 
						+ \gamma(3, 4) + \lambda_0 \text{log}\left( P(c_3^3 \mid c_2^2) \right) 
						+ \gamma(2, 3) + \gamma(3, 4) \right\} \cdot  \alpha[2][1] \nonumber \\
		&= \text{exp} \left\{ \lambda_0 \text{log}\left( P(<eos> \mid c_3^3) \right) 
						+ \gamma(3, 4) + \lambda_0 \text{log}\left( P(c_3^3 \mid c_2^2) \right) 
						+ \gamma(2, 3) + \gamma(3, 4) + \lambda_0 \text{log}\left( P(c_2^2 \mid c_1^1) \right) 
						+ \gamma(1, 2) + \gamma(2, 1) + \lambda_0 \text{log}\left( P(c_1^1 \mid \text{<bos>}) \right) 
						+ \gamma(0, 1) + \gamma(1, 2) \right\} \nonumber \\
		&= P(<eos> \mid c_3^3)^{\lambda_0}P(c_3^3 \mid c_2^2)^{\lambda_0}P(c_2^2 \mid c_1^1)^{\lambda_0}P(c_1^1 \mid \text{<bos>})^{\lambda_0} \nonumber \\ 
		&+ \text{exp}(2\gamma(3, 4))
		+ \text{exp}(2\gamma(2, 3))
		+ \text{exp}(2\gamma(1, 2))
		+ \text{exp}(\gamma(0, 1))
	\end{align}\
$$

一方、このケースで式(1)を展開すると以下のようになります。

$$
	\begin{align}
		P_{\text{CONC}}(\boldsymbol y \mid \boldsymbol x) &＝ P_{\text{CRF}}(\boldsymbol y \mid \boldsymbol x;\boldsymbol \Lambda)P_{\text{NPYLM}}(\boldsymbol y, \boldsymbol x;\boldsymbol \Theta)^{\lambda_0} \nonumber \\
		&＝ P(<eos> \mid c_3^3)^{\lambda_0}P(c_3^3 \mid c_2^2)^{\lambda_0}P(c_2^2 \mid c_1^1)^{\lambda_0}P(c_1^1 \mid \text{<bos>})^{\lambda_0} \nonumber\\ 
		&+ \text{exp}(\gamma(3, 4))
		+ \text{exp}(\gamma(2, 3))
		+ \text{exp}(\gamma(1, 2))
		+ \text{exp}(\gamma(0, 1))
	\end{align}\
$$

式(8)と式(9)をよく見比べると、式(8)では$\gamma$が2回出現しているため、前向き確率の計算時にモデルの統合が正しく行われていないことがわかります。

論文の以下の文はこのことを言っています。

>Markovモデルおよびsemi-Markovモデルそれぞれで特定のパス上のポテンシャルをそれぞれ一度ずつ足しあわせた形になっているはずであるが，(5) 式のように足しあわせて行くと，それぞれ γ(s, t) が二回ずつ現れることが分かる．この分解を見てもモデルの統合が正しくないことが直感的に理解できる．

このようにモデル統合に誤りがあるため、このままでは学習を正しく行えません。

## 正しいモデル変換

修正版のNPYCRFでは、任意の先行単語から単語$c_s^{t-1}$への遷移確率が$\gamma(s, t)$に対応していると考えます。

$$
	\begin{align}
		P(c_s^{t-1} \mid \cdot) = \frac{\text{exp}(\gamma(s, t))}{Z(x)}
	\end{align}\
$$

$Z(x)$はCRFの規格化定数（全経路のコストの総和）です。

さらに、前向き確率を以下のように表します。

$$
	\begin{align}
		\alpha[t][k] = \sum_{j=1}^{t-k} \text{exp} 
			\left\{ 
				\lambda_0 \text{log}\left( P(c_{t-k+1}^t \mid c_{t-k-j+1}^{t-k}) \right) 
				+ \gamma(t-k+1, t+1)
			\right\} \cdot \alpha[t-k][j]
	\end{align}\
$$

こうすることで前向き確率を終端まで計算した時に$P(c_{t-k+1}^t \mid c_{t-k-j+1}^{t-k})$と$\gamma(t-k+1, t+1)$が1回ずつ出現するため、モデル統合として辻褄が合っていることになります。

## CRFの素性

CRFの$k$番目の素性関数を$\pi_k(\boldsymbol x, y_{t-1}, y_t)$で表します。

素性関数は文字列$\boldsymbol x$と位置$t, t-1$のCRFの2値ラベル$y_t, y_{t-1}$が特定の組み合わせの時に1を返し、そうでない場合に0を返す関数になっています。

NPYLMでは以下の6種類の素性を考えます。

- $t-2, t-1, t, t+1, t+2$の位置の文字unigram
- $t-2, t-1, t, t+1$の位置の文字bigram
- $t-2, t-1, t, t+1$の位置について、$x_t = x_{t+1}$かどうか
- $t-3, t-2, t-1, t, t+1$の位置について、$x_t = x_{t+2}$かどうか
- $t$の位置の文字種のunigram
- $t$の位置の文字種のbigram

たとえば文字unigram素性として以下のようなものが考えられます。

$$
	\begin{align}
		\pi_k(\boldsymbol x, y_{t-1}, y_t) = 
			\begin{cases}
				1 & x_t\text{が"あ"} \land y_tが1 \land y_{t-1}が0 \\
				0 & それ以外
			\end{cases} \nonumber \\
	\end{align}\
$$

また、位置$t$のラベル$y_t$にのみ依存する素性関数$\pi_k(\boldsymbol x, y_t)$も用意します。

用いる素性の例をいくつか挙げておきます。（素性番号は適当です）

$$
	\begin{align}
		\pi_{33487}(\boldsymbol x, y_{t-1}, y_t) &= 
			\begin{cases}
				1 & x_{t-1}\text{が"機"} \land x_t\text{が"械"} \land y_tが1 \land y_{t-1}が0 \\
				0 & それ以外
			\end{cases} \nonumber \\

		\pi_{98}(\boldsymbol x, y_{t-1}, y_t) &= 
			\begin{cases}
				1 & x_{t-1} = x_t \\
				0 & それ以外
			\end{cases} \nonumber \\

		\pi_{128734}(\boldsymbol x, y_t) &= 
			\begin{cases}
				1 & \text{type}(x_t) = \text{"ひらがな"} \\
				0 & それ以外
			\end{cases} \nonumber \\
	\end{align}\
$$

学習時にはデータセットの各文から上記6通りの素性を展開し、それぞれの素性関数について重みを一つ用意するため、CRFのパラメータ数は数億を超えます。

分かりやすさのため$\pi_k(\boldsymbol x, y_{t-1}, y_t)$と表記しましたが、実際はラベルの値$i,j$と位置$t$を分けた$\pi_k(\boldsymbol x, i, j, t)$の形で素性関数を考えます。

## 周辺確率の計算

オリジナル版のNPYCRFではセミマルコフモデルのノード間の遷移確率をCRFのパスのコストに変換してCRFのパラメータを更新しています。

しかし、CRFのパラメータの勾配計算には、各素性の文字列上の各位置での発火の期待値さえ求めれば充分であるため、NPYLMの情報をマルコフモデルに変換せずに直接CRFの勾配を計算することができます。

素性関数$\pi_k$に対応する重み$\lambda_k$の、対数尤度$\cal{L}(\boldsymbol x, \boldsymbol y)$に対する勾配は以下のように求められます。

$$
	\begin{align}
		\frac{\partial \cal{L}(\boldsymbol x, \boldsymbol y)}{\partial \lambda_k} = 
			\sum_{t=1}^N \left\{ 
					\pi_k(\boldsymbol x, y_t, y_{t+1}, t + 1) - \sum_{i \in \{0,1\}}\sum_{j \in \{0,1\}}P(y_t=i, y_{t+1}=j \mid \boldsymbol x) \pi_k(\boldsymbol x, i, j, t + 1)
				\right\}
	\end{align}\
$$

$P(y_t=i, y_{t+1}=j \mid \boldsymbol x)$は、与えられた$\boldsymbol x$に対して現在のモデルでラベルを予測した時、位置$t$のラベルが$i$で位置$t+1$のラベルが$j$になる確率（周辺確率）を表しています。

そのため、各$m, i, j$について$P(y_t=i, y_{t+1}=j \mid \boldsymbol x)$が計算できれば$\lambda_0$の勾配を計算することができます。

### 単語の周辺確率

$P(y_t=i, y_{t+1}=j \mid \boldsymbol x)$を求めるには、まず単語の周辺確率を求める必要があります。

まず、$c_{t-k-j+1}^{t-k}$から$c_{t-k+1}^t$への遷移を考えると、この周辺確率は以下のように表されます。

$$
	\begin{align}
		P(c_{t-k-j+1}^{t-k},  c_{t-k+1}^t \mid \boldsymbol x) = 
		\frac {
			\alpha[t-k][j] \cdot \beta[t][k] \cdot \text{exp}\left[ \lambda_0 \text{log}\left(P(c_{t-k-j+1}^{t-k} \mid c_{t-k+1}^t)\right) + \gamma(t-k+1, t+1)\right]
		}{
			Z(x)^*
		}
	\end{align}\
$$

$\beta[t][k]$は以下で表される後向き確率です。

$$
	\begin{align}
		\beta[t][k] = \sum_{j=1}^{N-t} \text{exp}\left[
				\lambda_0 \text{log}\left(P(c_{t+1}^{t+j} \mid c_{t-k+1}^t)\right) + \gamma(t+1, t+j+1)
			\right] \cdot \beta[t+j][j]
	\end{align}\
$$

式(13)から単語の周辺確率を求めます。

$$
	\begin{align}
		P_{\text{CONC}}(c_{t-k+1}^t \mid \boldsymbol x) &= \sum_{j} P(c_{t-k-j+1}^{t-k},  c_{t-k+1}^t \mid \boldsymbol x)  \nonumber \\
		&= \sum_{j} \frac {
				\alpha[t-k][j] \cdot \beta[t][k] \cdot \text{exp}\left[ \lambda_0 \text{log}\left(P(c_{t-k-j+1}^{t-k} \mid c_{t-k+1}^t)\right) + \gamma(t-k+1, t+1)\right]
			}{
				Z(x)^*
			} \nonumber \\
		&= \frac{\beta[t][k]}{Z(x)^*} \sum_{j} \alpha[t-k][j] \cdot \beta[t][k] \cdot \text{exp}\left[ \lambda_0 \text{log}\left(P(c_{t-k-j+1}^{t-k} \mid c_{t-k+1}^t)\right) + \gamma(t-k+1, t+1)\right] \nonumber \\
		&= \frac{\alpha[t][k] \cdot \beta[t][k]}{Z(x)^*} 
	\end{align}\
$$

この式(15)を用いて$P(y_t=i, y_{t+1}=j \mid \boldsymbol x)$の各ラベル$i,j$の組み合わせ（`1-1`、`1-0`、`0-1`、`0-0`の4通り）について周辺確率を計算します。

### `1-1`の場合

$y_t=1, y_{t+1}=1$となるケースは$c_t^t$が単語になるため、周辺確率は以下のように求めます。

$$
	\begin{align}
		P(y_t=1, y_{t+1}=1 \mid \boldsymbol x) = P_{\text{CONC}}(c_t^t \mid \boldsymbol x)
	\end{align}\
$$

### `1-0`の場合

$y_{t}=1, y_{t+1}=0$となるケースは、位置$t$から任意の長さの単語が始まることを意味しているため、周辺確率は以下のように求めます。

$$
	\begin{align}
		P(y_t=1, y_{t+1}=0 \mid \boldsymbol x) 
		&= \sum_{j=2} P(y_t=1, y_{t+1}=0, ..., y_{t+j}=1 \mid \boldsymbol x) \nonumber \\
		&= \sum_{j=1} P_{\text{CONC}}(c_t^{t+j} \mid \boldsymbol x)
	\end{align}\
$$

ただし$...$の範囲のラベルの値はすべて$0$です。

### `0-1`の場合

$y_{t}=0, y_{t+1}=1$となるケースは、任意の長さの単語が位置$t$で終了し、 位置$t+1$から新しい単語が始まることを意味しているため、周辺確率は以下のように求めます。

$$
	\begin{align}
		P(y_t=0, y_{t+1}=1 \mid \boldsymbol x) 
		&= \sum_{j=1} P(y_{t-j}=1, ...,  y_{t}=0, y_{t+1}=1 \mid \boldsymbol x) \nonumber \\
		&= \sum_{j=1} P_{\text{CONC}}(c_{t-j}^{t} \mid \boldsymbol x)
	\end{align}\
$$

### `0-0`の場合

$y_{t}=0, y_{t+1}=0$となるケースは、位置$t$と位置$t+1$がともに単語の途中であることを意味しているため、周辺確率は以下のように求めます。

$$
	\begin{align}
		P(y_t=0, y_{t+1}=0 \mid \boldsymbol x) 
		&= \sum_{j=1}\sum_{k=2} P(y_{t-j}=1, ...,  y_{t}=0, y_{t+1}=0, ..., y_{t+k} \mid \boldsymbol x) \nonumber \\
		&= \sum_{j=1}\sum_{k=1} P_{\text{CONC}}(c_{t-j}^{t+k} \mid \boldsymbol x)
	\end{align}\
$$

ただし、$P(y_t=i, y_{t+1}=j \mid \boldsymbol x)$の各ラベル$i,j$の組み合わせは4通りしか存在しないため、`1-1`、`1-0`、`0-1`のケースを求めれば`0-0`のケースは以下のように求めることができます。

$$
	\begin{align}
		P(y_t=0, y_{t+1}=0 \mid \boldsymbol x) = 1 - P(y_t=1, y_{t+1}=1 \mid \boldsymbol x) - P(y_t=1, y_{t+1}=0 \mid \boldsymbol x) - P(y_t=0, y_{t+1}=1 \mid \boldsymbol x)
	\end{align}\
$$

### 3-gramの場合

### 実装

```cpp
void enumerate_marginal_p_path_given_sentence_using_p_substring(py_x, sentence_length, pc_x){
	py_x(0, 0, 0) = 0;
	py_x(0, 0, 1) = 0;
	py_x(0, 1, 0) = 0;
	py_x(0, 1, 1) = 1;
	for(t = 1;t <= sentence_length;t++){
		py_x(t, 1, 1) = compute_p_z_case_1_1(sentence_length, t, pc_x);
		py_x(t, 1, 0) = compute_p_z_case_1_0(sentence_length, t, pc_x);
		py_x(t, 0, 1) = compute_p_z_case_0_1(sentence_length, t, pc_x);
		py_x(t, 0, 0) = std::max(0.0, 1.0 - py_x(t, 1, 1) - py_x(t, 1, 0) - py_x(t, 0, 1));
	}
}
double compute_p_z_case_1_1(sentence_length, t, pc_x){
	return pc_x(t, 1);
}
double compute_p_z_case_1_0(sentence_length, t, pc_x){
	if(t == sentence_length){
		return 0;
	}
	double p_1_0 = 0;
	for(int j = 2;j <= std::min(sentence_length - t + 1, _max_word_length);j++){
		p_1_0 += pc_x(t + j - 1, j);
	}
	return p_1_0;
}
double compute_p_z_case_0_1(sentence_length, t, pc_x){
	double p_0_1 = 0;
	for(int j = 2;j <= std::min(t, _max_word_length);j++){
		p_0_1 += pc_x(t, j);
	}
	return p_0_1;
}
double compute_p_z_case_0_0(sentence_length, t, pc_x){
	if(t == 1){
		return 0;
	}
	double p_0_0 = 0;
	for(int k = 1;k <= std::min(sentence_length - t, _max_word_length - 2);k++){
		for(int j = k + 2;j <= std::min(t + k, _max_word_length);j++){
			p_0_0 += pc_x(t + k, j);
		}
	}
	return p_0_0;
}
```

実装時には`0-0`のケースも式(19)に従って計算し、式(20)の結果と一致するかどうかを確かましょう。